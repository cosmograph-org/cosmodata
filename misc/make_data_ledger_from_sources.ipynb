{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a data ledger from code and data folders \n",
    "\n",
    "Here, we will make data descriptions (data url, name, description, etc.) from analyzing code and local data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, Tuple, List, Optional, Callable, Mapping, Optional\n",
    "import os\n",
    "import json\n",
    "from functools import partial, cached_property\n",
    "from collections import ChainMap\n",
    "\n",
    "import dol\n",
    "from lkj import clog\n",
    "from tabled import pandas_json_dumps\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataInfoDacc:\n",
    "    data_folders: Dict[str, str]\n",
    "    code_and_data_mapping: Dict[str, str]\n",
    "    code_store: Dict[str, str]\n",
    "    data_folder_name_filt: Callable[[str], bool] = (\n",
    "        lambda x: not x.endswith(' alias') and not x.endswith('.parquet')\n",
    "    )\n",
    "    exclude_data_path_patterns: Tuple[str, ...] = ()\n",
    "    max_data_path_levels: Optional[int] = None\n",
    "    problematic_files: Tuple[str, ...] = ()\n",
    "    output_json_dir: str = \"data_folders_info\"\n",
    "    verbose: bool = True\n",
    "\n",
    "    @cached_property\n",
    "    def flat_data_folder_reader(self):\n",
    "        return dol.FlatReader(\n",
    "            {k: dol.Files(path) for k, path in self.data_folders.items()}\n",
    "        )\n",
    "\n",
    "    @cached_property\n",
    "    def project_folders(self):\n",
    "        pairs = {(x[0], x[1].split('/')[0]) for x in self.flat_data_folder_reader}\n",
    "        return {project_name: project_group for project_group, project_name in pairs}\n",
    "\n",
    "    @cached_property\n",
    "    def data_folder_names(self) -> List[str]:\n",
    "        _data_folder_names = {x[1].split('/')[0] for x in self.flat_data_folder_reader}\n",
    "        return sorted(filter(self.data_folder_name_filt, _data_folder_names))\n",
    "\n",
    "    def sizes_of_files_paths(self, filepaths: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Return a dictionary mapping each file path to its size (in bytes),\n",
    "        sorted by size.\n",
    "        \"\"\"\n",
    "        sizes = {fp: os.path.getsize(fp) for fp in filepaths}\n",
    "        # Sort the dictionary by file size\n",
    "        return dict(sorted(sizes.items(), key=lambda x: x[1]))\n",
    "\n",
    "    def code_and_data_files(self, name: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Given a project name, determine the corresponding project group and data folder.\n",
    "        Collect the list of file paths (and filenames) within that project folder.\n",
    "        If there is an associated code file, include its name and contents.\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        exclude_pattern = re.compile('|'.join(self.exclude_data_path_patterns))\n",
    "\n",
    "        # Determine the project group from the mapping\n",
    "        project_group = self.project_folders[name]\n",
    "        project_folderpath = os.path.join(self.data_folders[project_group], name)\n",
    "\n",
    "        # Read the filenames in the project folder using dol.Files\n",
    "        t = list(\n",
    "            dol.filt_iter(\n",
    "                dol.Files(project_folderpath, max_levels=self.max_data_path_levels),\n",
    "                filt=lambda x: exclude_pattern.match(x) is None,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Construct full file paths and get their sizes\n",
    "        data_filepaths_with_sizes = self.sizes_of_files_paths(\n",
    "            [os.path.join(project_folderpath, fn) for fn in t]\n",
    "        )\n",
    "        data_filenames = [\n",
    "            os.path.basename(fp) for fp in data_filepaths_with_sizes.keys()\n",
    "        ]\n",
    "\n",
    "        result: Dict[str, Any] = {\n",
    "            \"project_group\": project_group,\n",
    "            \"data_filenames\": data_filenames,\n",
    "            \"data_filepaths\": data_filepaths_with_sizes,\n",
    "        }\n",
    "\n",
    "        # If there is an associated code file, include its details\n",
    "        code_file = self.code_and_data_mapping.get(name)\n",
    "        if code_file:\n",
    "            result.update(\n",
    "                {\n",
    "                    \"code_file\": code_file,\n",
    "                    \"code_contents\": self.code_store.get(code_file),\n",
    "                }\n",
    "            )\n",
    "        return result\n",
    "\n",
    "    def table_info(self, filepath: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Attempts to load the table at a given filepath using tabled.get_table.\n",
    "        Returns a dictionary with the table shape and first row (if possible),\n",
    "        or None if the file cannot be processed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import tabled\n",
    "\n",
    "            df = tabled.get_table(filepath)\n",
    "            # Convert first row to dict if possible\n",
    "            first_row = df.iloc[0]\n",
    "            if hasattr(first_row, \"to_dict\"):\n",
    "                first_row = first_row.to_dict()\n",
    "            return {\n",
    "                \"shape\": df.shape,\n",
    "                \"first_row\": first_row,\n",
    "            }\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def gather_info_for_name(self, name: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        For a given project name, gather detailed information including:\n",
    "         - Data file paths and sizes\n",
    "         - Code file and its content (if available)\n",
    "         - The number of data files\n",
    "         - Table information for each data file (unless it is in the problematic list)\n",
    "\n",
    "        Uses the clog utility for logging based on the verbose flag.\n",
    "        \"\"\"\n",
    "        _clog: Callable[..., None] = clog(self.verbose)\n",
    "        _clog(f\"Getting info for {name}\")\n",
    "\n",
    "        info = self.code_and_data_files(name)\n",
    "        info['num_of_data_files'] = len(info['data_filepaths'])\n",
    "\n",
    "        def tables_info_generator():\n",
    "            for filepath in info['data_filepaths']:\n",
    "                if filepath in self.problematic_files:\n",
    "                    _clog(\"     ---> Skipping\", filepath)\n",
    "                    continue\n",
    "                _clog(\"   Getting table info for\", filepath)\n",
    "                yield os.path.basename(filepath), self.table_info(filepath)\n",
    "\n",
    "        info['tables_info'] = dict(tables_info_generator())\n",
    "        return info\n",
    "\n",
    "    @cached_property\n",
    "    def project_info_store(self) -> dol.Store:\n",
    "        \"\"\"\n",
    "        Constructs and returns a project info store using dol.\n",
    "        The store wraps a JSON key-value store and uses the gather_info_for_name method\n",
    "        as a missing key callback to compute project information on demand.\n",
    "        \"\"\"\n",
    "        # Wrap a TextFiles-based store for JSON files with proper encoding/decoding\n",
    "        JsonFiles = dol.wrap_kvs(\n",
    "            dol.TextFiles,\n",
    "            value_encoder=partial(pandas_json_dumps, indent=2),\n",
    "            value_decoder=json.loads,\n",
    "            key_codec=dol.KeyCodecs.suffixed(suffix='.json'),\n",
    "        )\n",
    "\n",
    "        json_store = dol.mk_dirs_if_missing(\n",
    "            dol.cached_keys(\n",
    "                JsonFiles(self.output_json_dir),\n",
    "                keys_cache=tuple(self.data_folder_names),\n",
    "            )\n",
    "        )\n",
    "        # json_store = dol.mk_dirs_if_missing(JsonFiles(self.output_json_dir))\n",
    "        # Attach a missing-key callback that gathers info for a project name\n",
    "\n",
    "        def gather_info_and_save_to_store(store, k):\n",
    "            info = self.gather_info_for_name(k)\n",
    "            store[k] = info\n",
    "            return info\n",
    "\n",
    "        return dol.add_missing_key_handling(\n",
    "            json_store,\n",
    "            missing_key_callback=gather_info_and_save_to_store,\n",
    "        )\n",
    "\n",
    "\n",
    "def code_store_from_code_folders(\n",
    "    code_folders: Mapping, *, exclude_keys=(), extra_files: Mapping = ()\n",
    "):\n",
    "    PyStore = dol.Pipe(\n",
    "        dol.TextFiles,\n",
    "        dol.filt_iter(\n",
    "            filt=lambda x: x.endswith('py')\n",
    "            and not x.startswith('_')\n",
    "            and x not in exclude_keys\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    code_store = ChainMap(*map(PyStore, code_folders.values()), dict(extra_files))\n",
    "    return code_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_folders= {\n",
    "    'imbed_data_prep': __import__('imbed_data_prep').__path__[0],\n",
    "}\n",
    "data_folders = {\n",
    "    'imbed_saves': \"/Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves\",\n",
    "    'figiri': \"/Users/thorwhalen/Dropbox/_odata/figiri/\"\n",
    "}\n",
    "\n",
    "\n",
    "code_and_data_mapping = {\n",
    "    'eurovis': 'eurovis.py',\n",
    "    'github_repos': 'github_repos.py',\n",
    "    'hcp': 'hcp.py',\n",
    "    'lmsys-chat-1m': 'lmsys_ai_conversations.py',\n",
    "    'prompt-injections': 'prompt_injections.py',\n",
    "    'twitter_sentiment': 'twitter_sentiment.py',\n",
    "    'wildchat': 'wildchat.py',\n",
    "    'wordnet_words': 'wordnet_words.py',\n",
    "}\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "exclude_keys = ['arxiv.py', 'ultra_chat.py', 'embeddings_of_aggregations.py']\n",
    "extra_files = {\n",
    "    'xv.data_access': Path(__import__('xv').data_access.__file__).read_text(),\n",
    "}\n",
    "code_store = code_store_from_code_folders(code_folders, exclude_keys=exclude_keys)\n",
    "\n",
    "exclude_data_path_patterns = (\n",
    "    r\".*tmp\\/.*\",\n",
    "    r\".*embeddings_chunks\\/.*\",\n",
    "    r\".*flat_en_embeddings\\/.*\",\n",
    ")\n",
    "problematic_files = [\n",
    "    \"/Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/wildchat/embeddings.parquet\",\n",
    "]\n",
    "\n",
    "list(code_store)\n",
    "\n",
    "dacc = DataInfoDacc(\n",
    "    data_folders, \n",
    "    code_and_data_mapping=code_and_data_mapping, \n",
    "    code_store=code_store, \n",
    "    exclude_data_path_patterns=exclude_data_path_patterns,\n",
    "    problematic_files=problematic_files\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eurovis',\n",
       " 'github_repos',\n",
       " 'harris_vs_trump',\n",
       " 'hcp',\n",
       " 'lmsys-chat-1m',\n",
       " 'new_years_resolutions',\n",
       " 'prompt-injections',\n",
       " 'quotes',\n",
       " 'spotify_playlists',\n",
       " 'twitter_sentiment',\n",
       " 'wildchat',\n",
       " 'wordnet_words']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dacc.project_info_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting info for eurovis\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/cluster_13_labels.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/cluster_21_labels.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/clusters_df.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/planar_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/oa_embeddings_batch_keys.pkl\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/raw_data.csv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/embeddable.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/merged_artifacts.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/embeddings_df.parquet\n",
      "Getting info for github_repos\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/github_repos/cluster_indices..parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/github_repos/planar_embeddings..parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/github_repos/github_repo_for_cosmos.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/github_repos/github-repos.parquet\n",
      "Getting info for harris_vs_trump\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/harris_vs_trump/harris_vs_trump_debate__conversation_links.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/harris_vs_trump/https/raw.githubusercontent.com_f/jumonlala_f/harris_trump_debate_f/refs_f/heads_f/main_f/data_f/harris_trump.csv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/harris_vs_trump/harris_vs_trump_animated.mov\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/harris_vs_trump/openai_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/harris_vs_trump/harris_vs_trump_debate.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/harris_vs_trump/harris_vs_trump_debate_with_extras.parquet\n",
      "Getting info for hcp\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/hcp_closest_tenth_knn_graph.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/aggregate_titles_embeddings_umap_2d.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/publications-hcp2.tsv.zip\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/planar_cited_mean_embeddings.tsv.zip\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/planar_cited_normalized_mean_embeddings.tsv.zip\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/aggregate_titles_embeddings_umap_2d_with_info.tsv.zip\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/publications-hcp2-umap.tsv.zip\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/aggregate_titles_embeddings_umap_2d_with_info.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/citation-links-hcp3.tsv.zip\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/hcp_closest_third_knn_graph.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/planar_cited_mean_embeddings.tsv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/planar_cited_normalized_mean_embeddings.tsv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/aggregate_titles_embeddings_umap_2d_with_info.tsv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/publications-hcp2-umap.tsv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/citation-links-hcp3.tsv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/hcp_nearest_neighbors_graph.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/publications-hcp3__.tsv.zip\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/publications-hcp3-with_titles_aggregates.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/publications-hcp3__.tsv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/aggregate_titles_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/hcp/publications-hcp2-embeddings.parquet\n",
      "Getting info for lmsys-chat-1m\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/Icon\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/pca_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thorwhalen/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator IncrementalPCA from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/thorwhalen/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator KMeans from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/planar_embeddings_first_forth.npy\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/pca500.pkl\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/kmeans_7_clusters_indices.pkl\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/kmeans_14_clusters_indices.pkl\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/kmeans_28_clusters_indices.pkl\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/dbscan_7_kmeans.pkl\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/planar_embeddings_pca500.npy\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/ncvis_planar_pca500_embeddings.npy\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/num_of_tokens.pkl\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/planar_embeddings_of_grouped_conversations_with_metadata.tsv.zip\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/planar_embeddings_grouped.tsv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/conversation_paths.tsv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/planar_embeddings_grouped.pkl\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/planar_embeddings_of_grouped_conversations_with_metadata.tsv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/planar_embeddings_for_a_forth_of_data.tsv.zip\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/planar_embeddings_for_a_forth_of_data.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/planar_embeddings_for_a_forth_of_data.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thorwhalen/Dropbox/py/proj/i/dol/dol/util.py:1820: DtypeWarning: Columns (0,3,5,8,9,10,11,12,13,14,15,16,17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return reader(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/flat_en.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/lmsys_with_planar_embeddings_pca500.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/flat_en_embeddings_pca100.npy\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/flat_en_conversation_grouped_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/lmsys-chat-1m/pca500_embeddings.npy\n",
      "Getting info for new_years_resolutions\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/new_years_resolutions/new_years_resolutions_notebook.pdf\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/new_years_resolutions/with_hashtags/new_years_resolutions_prepped.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/new_years_resolutions/without_hashtags/new_years_resolutions_prepped.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/new_years_resolutions/openai_topics_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/new_years_resolutions/without_hashtags/various_scatter_plots.pdf\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/new_years_resolutions/with_hashtags/various_scatter_plots.pdf\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/new_years_resolutions/various_scatter_plots.pdf\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/new_years_resolutions/various_scatter_plots_23.pdf\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/new_years_resolutions/openai_tweet_text_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/new_years_resolutions/openai_text_embeddings.parquet\n",
      "Getting info for prompt-injections\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/prompt-injections/planar_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/prompt-injections/prompt_injection_w_umap_embeddings.tsv\n",
      "Getting info for quotes\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/quotes/micheleriva_1638_quotes/micheleriva_1638_quotes_planar_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/quotes/micheleriva_1638_quotes/micheleriva_1638_quotes_and_embeddings.parquet\n",
      "Getting info for spotify_playlists\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/superhero_laundry_day.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/holiday_playlist/holiday_songs_spotify_with_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/holiday_playlist/holiday_songs_spotify_with_embeddings.csv\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/https/raw.githubusercontent.com_f/thorwhalen_f/sung_content_f/main_f/parquet_f/greatest_500_songs.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/tables/greatest_500_songs.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/https/raw.githubusercontent.com_f/thorwhalen_f/sung_content_f/main_f/parquet_f/top_500_last_decade.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/https/raw.githubusercontent.com_f/thorwhalen_f/sung_content_f/main_f/parquet_f/spotify_top_500_streamed.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/tables/top_500_last_decade.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/tables/spotify_top_500_streamed.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/https/raw.githubusercontent.com_f/thorwhalen_f/sung_content_f/main_f/parquet_f/rolling_stone_500_greatest_songs.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/tables/rolling_stone_500_greatest_songs.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/https/raw.githubusercontent.com_f/thorwhalen_f/sung_content_f/main_f/parquet_f/over_500_million_streams.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/spotify_playlists/tables/over_500_million_streams.parquet\n",
      "Getting info for twitter_sentiment\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/twitter_sentiment/twitter_sentiment.zip\n",
      "Getting info for wildchat\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/wildchat/clusters.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/wildchat/planar_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/wildchat/metadata.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/wildchat/scores.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/wildchat/segments.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/wildchat/embeddable_df.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/wildchat/wildchat_train.parquet\n",
      "     ---> Skipping /Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/wildchat/embeddings.parquet\n",
      "Getting info for wordnet_words\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/substance_meronyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/substance_holonyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/in_region_domains.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/region_domains.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/in_usage_domains.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/usage_domains.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/causes.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/entailments.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/attributes.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/member_meronyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/member_holonyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/instance_hypernyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/instance_hyponyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/in_topic_domains.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/topic_domains.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/part_meronyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/part_holonyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/verb_groups.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/similar_tos.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/umap_embeddings.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/hypernyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/root_hypernyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/link_data/hyponyms.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/word_and_synset.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/wordnet_collection_meta.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/wordnet_feature_meta.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/wordnet_metadata.parquet\n",
      "   Getting table info for /Users/thorwhalen/Dropbox/_odata/figiri/wordnet_words/words_embeddings.parquet\n"
     ]
    }
   ],
   "source": [
    "# This will go through all the keys, thereby computing all the project info\n",
    "# and saving it to disk\n",
    "for k, v in dacc.project_info_store.items():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275301"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import lkj \n",
    "\n",
    "\n",
    "t = dict(dacc.project_info_store)\n",
    "tt = lkj.truncate_dict_values(t, max_list_size=6)\n",
    "pathlib.Path('data_folders_info.json').write_text(json.dumps(tt, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(859488, 275301)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lkj\n",
    "\n",
    "tt = lkj.truncate_dict_values(t, max_list_size=6)\n",
    "\n",
    "len(json.dumps(t, indent=2)), len(json.dumps(tt, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84309"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Move this to lkj or context making package\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def truncate_big_dicts(d, max_length_of_dict=6, yield_condition: Callable = lambda x: True, *, middle_element={'...': '...'}):\n",
    "    \"\"\"\n",
    "    Recursively traverse the dictionary and replace any \"big\" dictionaries with a truncated version.\n",
    "    \n",
    "    A dictionary is considered \"big\" if its length exceeds max_length_of_dict and yield_condition(dict) returns True.\n",
    "    For such dictionaries, only the first (max_length_of_dict - 1) entries and the last entry are kept.\n",
    "    A middle_element (a dictionary) is inserted between these entries to indicate omitted content.\n",
    "    \n",
    "    If yield_condition returns False for a given dictionary, its contents are recursed normally without truncation.\n",
    "    \n",
    "    Parameters:\n",
    "      d (dict): The dictionary to process.\n",
    "      max_length_of_dict (int): The maximum allowed number of entries in a dictionary before it is truncated.\n",
    "      yield_condition (Callable): A function that takes a dictionary and returns True if it should be truncated,\n",
    "                                  and False if it should be left as is.\n",
    "      middle_element (dict): A dictionary to insert in the middle of the truncated dictionary, to signal omissions.\n",
    "    \n",
    "    Returns:\n",
    "      A new dictionary with the same structure as d, with any \"big\" dictionaries truncated according to the rule.\n",
    "    \"\"\"\n",
    "    # If the input is not a dictionary, return it unchanged.\n",
    "    if not isinstance(d, dict):\n",
    "        return d\n",
    "\n",
    "    # First, recursively process all values.\n",
    "    processed = {key: truncate_big_dicts(value, max_length_of_dict, yield_condition, middle_element=middle_element)\n",
    "                 for key, value in d.items()}\n",
    "    \n",
    "    # Check if the current dictionary is \"big\" and meets the condition to be truncated.\n",
    "    if len(processed) > max_length_of_dict and yield_condition(processed):\n",
    "        # Convert items to a list to preserve order.\n",
    "        items = list(processed.items())\n",
    "        truncated = {}\n",
    "        # Retain the first (max_length_of_dict - 1) key-value pairs.\n",
    "        for key, value in items[:max_length_of_dict - 1]:\n",
    "            truncated[key] = value\n",
    "        # Insert the middle element to indicate omitted entries.\n",
    "        truncated.update(middle_element)\n",
    "        # Append the last key-value pair.\n",
    "        last_key, last_value = items[-1]\n",
    "        truncated[last_key] = last_value\n",
    "        return truncated\n",
    "    else:\n",
    "        return processed\n",
    "\n",
    "def values_are_all_numerical(d):\n",
    "    return all(isinstance(v, (int, float)) for v in d.values())\n",
    "\n",
    "ttt = truncate_big_dicts(tt, yield_condition=values_are_all_numerical)\n",
    "\n",
    "pathlib.Path('data_folders_info.json').write_text(json.dumps(ttt, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('new_years_resolutions',\n",
       " 'tables_info',\n",
       " 'openai_topics_embeddings.parquet',\n",
       " 'first_row')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go through a dict recursively and find and yield dicts that are more than 1000 items long\n",
    "def find_big_dicts(d, max_length=1000, path=()):\n",
    "    if not isinstance(d, dict):\n",
    "        return\n",
    "    if len(d) > max_length:\n",
    "        yield path, d\n",
    "    for k, v in d.items():\n",
    "        yield from find_big_dicts(v, max_length, path=path + (k,))\n",
    "\n",
    "k, w = next(find_big_dicts(ttt))\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting base urls from code\n",
    "\n",
    "Make a function that will be given code (string) and figure out where the raw (seed) data is downloaded from.\n",
    "\n",
    "Yeah, a pretty hard NLP problem. But now we have LLMs, so..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached json schema\n"
     ]
    }
   ],
   "source": [
    "import oa\n",
    "\n",
    "refresh_json_schema = False\n",
    "\n",
    "\n",
    "schema_description = \"\"\"\n",
    "    The output should include:\n",
    "    * \"url\" field that points to the raw data source if there is only one source\n",
    "    * \"urls\" (json object) field that is used when there are multiple sources for \n",
    "        various kinds of data is a json object where the keys are the names of the \n",
    "        data sources and the values are the urls\n",
    "    * \"url_type\": (string) field that indicates the type of the url (e.g. \"http\", \"file\", \"env_var\", \"config_key\")\n",
    "    * \"name\" (string) field that is a short name for the data set,\n",
    "    * \"description\" (string) field that is a longer description of the data set\n",
    "    * \"parameters\" (json object whose fields are names and values are descriptions) field\n",
    "        that describe the different parameters that are used in the data preperation process \n",
    "    * \"data_keys\" (json object) fields which should indicate what kind of data artifacts \n",
    "        are created, or used in the data preperation process. This is often in the form of a \n",
    "        string that refers to a relative path, or other identifier.\n",
    "    * \"functions\" (jso object) field containing python functions or methods that are \n",
    "        created in the data preperation process.\n",
    "    \"\"\"\n",
    "\n",
    "if refresh_json_schema:\n",
    "    print('Recomputing a json schema')\n",
    "    from pprint import pprint\n",
    "\n",
    "    json_schema = oa.tools.infer_schema_from_verbal_description(schema_description)\n",
    "\n",
    "    print(\"--------------------\")\n",
    "    print(\n",
    "        \"Make sure to copy the following json schema into the code defining json_schema if you want to reuse it!!!\"\n",
    "    )\n",
    "    print(\"--------------------\")\n",
    "    pprint(json_schema)\n",
    "\n",
    "\n",
    "else:\n",
    "    print('using cached json schema')\n",
    "    json_schema = {\n",
    "        'name': 'data_prep_code_analysis',\n",
    "        'properties': {\n",
    "            'name': {'type': 'string'},\n",
    "            'url': {'type': 'string'},\n",
    "            'url_type': {\n",
    "                'enum': ['http', 'file', 'env_var', 'config_key'],\n",
    "                'type': 'string',\n",
    "            },\n",
    "            'urls': {'additionalProperties': {'type': 'string'}, 'type': 'object'},\n",
    "            'data_keys': {\n",
    "                'additionalProperties': {'type': 'string'},\n",
    "                'type': 'object',\n",
    "            },\n",
    "            'description': {'type': 'string'},\n",
    "            'functions': {\n",
    "                'additionalProperties': {'type': 'string'},\n",
    "                'type': 'object',\n",
    "            },\n",
    "            'parameters': {\n",
    "                'additionalProperties': {'type': 'string'},\n",
    "                'type': 'object',\n",
    "            },\n",
    "        },\n",
    "        'required': [\n",
    "            'name',\n",
    "            'url',\n",
    "            'urls',\n",
    "            'url_type',\n",
    "            'description',\n",
    "            'parameters',\n",
    "            'data_keys',\n",
    "            'functions',\n",
    "        ],\n",
    "        'type': 'object',\n",
    "    }\n",
    "\n",
    "analyze_data_prep_code = oa.prompt_json_function(\n",
    "    f\"\"\"\n",
    "    This python code should (but not necessarily) contain some data preperation code \n",
    "    that downloads some raw data and then processes it into a form that is ready for analysis.\n",
    "\n",
    "    You should study the code and make a json object that describes the main \"data feature\" \n",
    "    that are therein: The data source, the data preparation artifacts, and the main \n",
    "    python functions, methods, or constants that the code creates.\n",
    "    Note that sometimes the \"url\" or \"urls\" may not be actual http(s) urls, but could \n",
    "    also be filepaths, or the name(s) of environmental variables or configuration values\n",
    "    (often in all caps) that are used to locate the url(s) of the data source(s).\n",
    "    In this case, you should still use the \"url\" or \"urls\" fields, but the values should\n",
    "    be strings that are not valid urls, but rather the name of the filepath, \n",
    "    the environmental variable or configuration key.\n",
    "\n",
    "    Usually, if there is an http(s) url in the code, it is the main data source.\n",
    "\n",
    "    Note though, that some inputs that are given may not be actual data prep code.\n",
    "\n",
    "    In this case, you should just return an empty string for url and empty json objects \n",
    "    for urls, and in the description, you should indicate that this is not data prep code, \n",
    "    perhaps explaining why you think so. In this case start the description with \n",
    "    \"NOT DATA PREP CODE\", followed with your analysis/explication.\n",
    "\n",
    "    {schema_description}\n",
    "\n",
    "    {{python_code}}\n",
    "    \"\"\",\n",
    "    json_schema=json_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the code files we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_string_json_decode_it(x):\n",
    "    if isinstance(x, str):\n",
    "        import json \n",
    "        try:\n",
    "            return json.loads(x)\n",
    "        except:\n",
    "            pass\n",
    "    return x\n",
    "\n",
    "def get_analysis_dict(code_key, py_file_contents):\n",
    "    \"\"\"Use this to extend the code_folders_analysis by doing\n",
    "    code_folders_analysis.extend(get_analysis_dict(code_key, py_file_contents))\n",
    "    or to replace an entry by doing\n",
    "    code_folders_analysis[i] = get_analysis_dict(code_key, py_file_contents)\n",
    "    \"\"\"\n",
    "    d = analyze_data_prep_code(py_file_contents)\n",
    "    if 'result' in d:\n",
    "        d['result'] = if_string_json_decode_it(d['result'])\n",
    "    d['code'] = code_key\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xv.data_access',\n",
       " 'prompt_injections.py',\n",
       " 'wordnet_words.py',\n",
       " 'hcp.py',\n",
       " 'jersey_laws.py',\n",
       " 'github_repos.py',\n",
       " 'wildchat.py',\n",
       " 'eurovis.py',\n",
       " 'lmsys_ai_conversations.py',\n",
       " 'twitter_sentiment.py']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dol \n",
    "from collections import ChainMap\n",
    "from pathlib import Path\n",
    "\n",
    "exclude = {'arxiv.py', 'ultra_chat.py', 'embeddings_of_aggregations.py'}\n",
    "PyStore = dol.Pipe(\n",
    "    dol.TextFiles, \n",
    "    dol.filt_iter(\n",
    "        filt=lambda x: x.endswith('py') and not x.startswith('_') and x not in exclude\n",
    "    ),\n",
    ")\n",
    "\n",
    "s = PyStore(__import__('imbed_data_prep').__path__[0])\n",
    "extra_files = {\n",
    "    'xv.data_access': Path(__import__('xv').data_access.__file__).read_text(),\n",
    "}\n",
    "\n",
    "code_store = ChainMap(\n",
    "    s,\n",
    "    extra_files\n",
    ")\n",
    "list(code_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_code_folders_analysis = False\n",
    "code_folders_analysis_save_path = 'code_folders_analysis.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 code folders analyzed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xv.data_access',\n",
       " 'prompt_injections.py',\n",
       " 'wordnet_words.py',\n",
       " 'hcp.py',\n",
       " 'jersey_laws.py',\n",
       " 'github_repos.py',\n",
       " 'wildchat.py',\n",
       " 'eurovis.py',\n",
       " 'lmsys_ai_conversations.py',\n",
       " 'twitter_sentiment.py']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "if refresh_code_folders_analysis:\n",
    "    \n",
    "    def analyze_data_prep_code_of_folder(code_store, verbose=True):\n",
    "        for code_key, py_file_contents in code_store.items():\n",
    "            if verbose:\n",
    "                print(f\"  Analyzing {code_key}\")\n",
    "            yield get_analysis_dict(code_key, py_file_contents)\n",
    "            \n",
    "    code_folders_analysis = list(analyze_data_prep_code_of_folder(code_store))\n",
    "\n",
    "    # sometimes the AI didn't manage to catch the urls, so we can manually add them here\n",
    "\n",
    "    backup_urls = {\n",
    "        'hcp.py': {\n",
    "            'url': 'HCP_PUBS_SRC_KEY',\n",
    "            'url_type': 'env_var',\n",
    "        },\n",
    "        'eurovis.py': {\n",
    "            'url': 'https://81593031860c2ee4ad53a08892f7e95d.r2.cloudflarestorage.com/cosmograph/projects/eurovis/raw_data.csv',\n",
    "            'url_type': 'http',\n",
    "            # 'filepath': '/Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/raw_data.csv'\n",
    "        },\n",
    "        # 'embeddings_of_aggregations.py': {\"NOT A DATA PREP MODULE\"}\n",
    "    }\n",
    "\n",
    "\n",
    "    for d in code_folders_analysis:\n",
    "        if not d.get('url') and not d.get('urls'):\n",
    "            if d['code'] in backup_urls:\n",
    "                d.update(backup_urls[d['code']])\n",
    "\n",
    "    json.dump(code_folders_analysis, open(code_folders_analysis_save_path, 'w'))\n",
    "\n",
    "else:\n",
    "    code_folders_analysis = json.load(open(code_folders_analysis_save_path))\n",
    "\n",
    "\n",
    "print(f\"{len(code_folders_analysis)} code folders analyzed\")\n",
    "[d['code'] for d in code_folders_analysis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(code_folders_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xv.data_access',\n",
       " 'prompt_injections.py',\n",
       " 'wordnet_words.py',\n",
       " 'hcp.py',\n",
       " 'jersey_laws.py',\n",
       " 'github_repos.py',\n",
       " 'wildchat.py',\n",
       " 'eurovis.py',\n",
       " 'lmsys_ai_conversations.py',\n",
       " 'twitter_sentiment.py']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_folders_analysis_by_key = {d['code']: d for d in code_folders_analysis}\n",
    "code_analysis_keys = list(code_folders_analysis_by_key)\n",
    "code_analysis_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting information from the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders = {\n",
    "    'imbed_saves': \"/Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves\",\n",
    "    'figiri': \"/Users/thorwhalen/Dropbox/_odata/figiri/\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code_and_data_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eurovis',\n",
       " 'github_repos',\n",
       " 'harris_vs_trump',\n",
       " 'hcp',\n",
       " 'lmsys-chat-1m',\n",
       " 'new_years_resolutions',\n",
       " 'prompt-injections',\n",
       " 'quotes',\n",
       " 'spotify_playlists',\n",
       " 'twitter_sentiment',\n",
       " 'wildchat',\n",
       " 'wordnet_words']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dol \n",
    "\n",
    "t = dol.FlatReader({k: dol.Files(path) for k, path in data_folders.items()})\n",
    "\n",
    "pairs = {(x[0], x[1].split('/')[0]) for x in t}\n",
    "data_folder_names = {x[1].split('/')[0] for x in t}\n",
    "data_folder_names = sorted(\n",
    "    filter(lambda x: not x.endswith(' alias') and not x.endswith('.parquet'), data_folder_names)\n",
    ")\n",
    "project_folders = {project_name: project_group for project_group, project_name in pairs}\n",
    "data_folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eurovis': 'eurovis.py',\n",
       " 'github_repos': 'github_repos.py',\n",
       " 'hcp': 'hcp.py',\n",
       " 'lmsys-chat-1m': 'lmsys_ai_conversations.py',\n",
       " 'prompt-injections': 'prompt_injections.py',\n",
       " 'twitter_sentiment': 'twitter_sentiment.py',\n",
       " 'wildchat': 'wildchat.py',\n",
       " 'wordnet_words': 'wordnet_words.py'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refresh_code_and_data_matching = False\n",
    "\n",
    "if refresh_code_and_data_matching:\n",
    "    import oa\n",
    "\n",
    "    f = oa.prompt_json_function(\n",
    "        \"\"\"\n",
    "    Here's a list of data prep code files and a list of data folders.\n",
    "                                \n",
    "    Give me a json object that maps the data folders to the relevant code file\n",
    "    that they use. Don't include those pairs that do not match.\n",
    "\n",
    "    data_folders: {data_folders}    \n",
    "    code_files: {code_files}\n",
    "    \"\"\",\n",
    "        # json_schema=\"the json schema could be just a json object with keys that are the \"\n",
    "        # \"data folder names and values that are the code files\",\n",
    "    )\n",
    "\n",
    "    code_and_data_mapping = f(data_folder_names, code_files=code_analysis_keys)['result']\n",
    "else:\n",
    "    code_and_data_mapping = {\n",
    "        'eurovis': 'eurovis.py',\n",
    "        'github_repos': 'github_repos.py',\n",
    "        'hcp': 'hcp.py',\n",
    "        'lmsys-chat-1m': 'lmsys_ai_conversations.py',\n",
    "        'prompt-injections': 'prompt_injections.py',\n",
    "        'twitter_sentiment': 'twitter_sentiment.py',\n",
    "        'wildchat': 'wildchat.py',\n",
    "        'wordnet_words': 'wordnet_words.py',\n",
    "    }\n",
    "\n",
    "code_and_data_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['harris_vs_trump', 'new_years_resolutions', 'quotes', 'spotify_playlists'],\n",
       " ['jersey_laws.py', 'xv.data_access'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_remaining_code_names = sorted(set(code_analysis_keys) - set(code_and_data_mapping.values()))\n",
    "_remaining_data_folders = sorted(set(data_folder_names) - set(code_and_data_mapping.keys()))\n",
    "_remaining_data_folders, _remaining_code_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to test\n",
    "assert project_folders['wildchat'] == 'imbed_saves'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather information for each project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project_group',\n",
       " 'data_filenames',\n",
       " 'data_filepaths',\n",
       " 'code_file',\n",
       " 'code_contents']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dol \n",
    "import os \n",
    "\n",
    "def code_and_data_files(name):\n",
    "    project_group = project_folders[name]\n",
    "    project_folderpath = os.path.join(data_folders[project_group], name)\n",
    "    t = dol.filt_iter(\n",
    "        dol.Files(project_folderpath, max_levels=0),\n",
    "    )\n",
    "    data_filepaths = [os.path.join(project_folderpath, fn) for fn in t]\n",
    "    data_filepaths = sizes_of_files_paths(data_filepaths)\n",
    "    data_filenames = list(map(os.path.basename, data_filepaths))\n",
    "\n",
    "    d = dict(\n",
    "        project_group=project_group, \n",
    "        data_filenames=data_filenames,\n",
    "        data_filepaths=data_filepaths\n",
    "    )\n",
    "\n",
    "    code_file = code_and_data_mapping.get(name, None)\n",
    "    if code_file:\n",
    "        d.update(code_file=code_file, code_contents=code_store[code_file])\n",
    "        \n",
    "    return d\n",
    "\n",
    "def sizes_of_files_paths(filepaths):\n",
    "    import os \n",
    "    sizes = {fp: os.path.getsize(fp) for fp in filepaths}\n",
    "    # sort by size\n",
    "    sizes = dict(sorted(sizes.items(), key=lambda x: x[1]))\n",
    "    return sizes\n",
    "\n",
    "# test\n",
    "t = code_and_data_files('hcp')\n",
    "list(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thorwhalen/Dropbox/py/proj/i/dol/dol/trans.py:3244: UserWarning: Encountered error checking if <function <lambda> at 0x137df5b40> can be a store method. Will return True, to not disrupt process, but you may want to check on this : len() takes no keyword arguments\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "def table_info(filepath):\n",
    "    import tabled\n",
    "\n",
    "    try:\n",
    "        df = tabled.get_table(filepath)\n",
    "        return dict(\n",
    "            shape=df.shape,\n",
    "            first_row=df.iloc[0],\n",
    "        )\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "problematic_files = tuple(\n",
    "    [\n",
    "        '/Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/wildchat/embeddings.parquet',\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def gather_info_for_name(name, skip=problematic_files, verbose=True):\n",
    "    _clog = clog(verbose)\n",
    "    _clog(f\"Getting info for {name}\")\n",
    "    d = code_and_data_files(name)\n",
    "    d['num_of_data_files'] = len(d['data_filepaths'])\n",
    "\n",
    "    def tables_info():\n",
    "        for filepath in d['data_filepaths']:\n",
    "            if filepath in skip:\n",
    "                _clog(\"     ---> Skipping\", filepath)\n",
    "                continue\n",
    "            _clog(\"   Getting table info for\", filepath)\n",
    "            yield os.path.basename(filepath), table_info(filepath)\n",
    "\n",
    "    d['tables_info'] = dict(tables_info())\n",
    "    return d\n",
    "\n",
    "\n",
    "from tabled import pandas_json_dumps\n",
    "import dol\n",
    "from functools import partial\n",
    "from lkj import clog\n",
    "\n",
    "\n",
    "JsonFiles = dol.wrap_kvs(\n",
    "    dol.TextFiles, \n",
    "    value_encoder=partial(pandas_json_dumps, indent=2), \n",
    "    value_decoder=json.loads,\n",
    "    key_codec=dol.KeyCodecs.suffixed(suffix='.json')\n",
    ")\n",
    "\n",
    "# json_store = dol.mk_dirs_if_missing(JsonFiles('data_folders_info'))\n",
    "project_info_store = dol.add_missing_key_handling(\n",
    "    dol.mk_dirs_if_missing(JsonFiles('data_folders_info')),\n",
    "    missing_key_callback=lambda store, k: gather_info_for_name(k),\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor to Dacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dacc.project_info_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spotify_playlists',\n",
       " 'hcp',\n",
       " 'prompt-injections',\n",
       " 'twitter_sentiment',\n",
       " 'new_years_resolutions',\n",
       " 'quotes',\n",
       " 'wordnet_words',\n",
       " 'eurovis',\n",
       " 'harris_vs_trump',\n",
       " 'wildchat',\n",
       " 'lmsys-chat-1m',\n",
       " 'github_repos']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eurovis/embeddings_df.parquet',\n",
       " 'eurovis/planar_embeddings.parquet',\n",
       " 'eurovis/clusters_df.parquet',\n",
       " 'eurovis/cluster_21_labels.parquet',\n",
       " 'eurovis/merged_artifacts.parquet',\n",
       " 'eurovis/embeddable.parquet',\n",
       " 'eurovis/raw_data.csv',\n",
       " 'eurovis/cluster_13_labels.parquet',\n",
       " 'eurovis/oa_embeddings_batch_keys.pkl',\n",
       " 'lmsys-chat-1m/flat_en_conversation_grouped_embeddings.parquet',\n",
       " 'lmsys-chat-1m/planar_embeddings_for_a_forth_of_data.parquet',\n",
       " 'lmsys-chat-1m/planar_embeddings_for_a_forth_of_data.tsv.zip',\n",
       " 'lmsys-chat-1m/Icon\\r',\n",
       " 'lmsys-chat-1m/planar_embeddings_grouped.tsv',\n",
       " 'lmsys-chat-1m/planar_embeddings_for_a_forth_of_data.tsv',\n",
       " 'lmsys-chat-1m/pca500.pkl',\n",
       " 'lmsys-chat-1m/planar_embeddings_grouped.pkl',\n",
       " 'lmsys-chat-1m/pca500_embeddings.npy',\n",
       " 'lmsys-chat-1m/kmeans_7_clusters_indices.pkl',\n",
       " 'lmsys-chat-1m/planar_embeddings_pca500.npy',\n",
       " 'lmsys-chat-1m/lmsys_with_planar_embeddings_pca500.parquet',\n",
       " 'lmsys-chat-1m/planar_embeddings_of_grouped_conversations_with_metadata.tsv.zip',\n",
       " 'lmsys-chat-1m/kmeans_14_clusters_indices.pkl',\n",
       " 'lmsys-chat-1m/num_of_tokens.pkl',\n",
       " 'lmsys-chat-1m/dbscan_7_kmeans.pkl',\n",
       " 'lmsys-chat-1m/planar_embeddings_of_grouped_conversations_with_metadata.tsv',\n",
       " 'lmsys-chat-1m/planar_embeddings_first_forth.npy',\n",
       " 'lmsys-chat-1m/flat_en.parquet',\n",
       " 'lmsys-chat-1m/ncvis_planar_pca500_embeddings.npy',\n",
       " 'lmsys-chat-1m/kmeans_28_clusters_indices.pkl',\n",
       " 'lmsys-chat-1m/pca_model.pkl',\n",
       " 'lmsys-chat-1m/conversation_paths.tsv',\n",
       " 'twitter_sentiment/twitter_sentiment.zip',\n",
       " 'github_repos/planar_embeddings..parquet',\n",
       " 'github_repos/cluster_indices..parquet',\n",
       " 'wildchat/embeddable_df.parquet',\n",
       " 'wildchat/planar_embeddings.parquet',\n",
       " 'wildchat/embeddings.parquet',\n",
       " 'wildchat/scores.parquet',\n",
       " 'wildchat/metadata.parquet',\n",
       " 'wildchat/wildchat_train.parquet',\n",
       " 'wildchat/segments.parquet',\n",
       " 'wildchat/clusters.parquet']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dol\n",
    "import tabled\n",
    "import re\n",
    "\n",
    "\n",
    "exclude_patterns = [\n",
    "    '.*flat_en_embeddings.*',  # folder containing chunks of data\n",
    "    'hcp/tmp/.*',  # folder containing chunks of data\n",
    "    '.*.pdf',  # pdf files\n",
    "    '.*.png',  # png files\n",
    "    '.*.mov',  # mov files\n",
    "    '.*\\ aliased',  # aliased folders\n",
    "\n",
    "]\n",
    "\n",
    "exclude_filt = dol.filt_iter(\n",
    "    filt=lambda x: not any(re.match(p, x) for p in exclude_patterns)\n",
    ")\n",
    "\n",
    "exclude_for_tables = [\n",
    "    '.*.zip'\n",
    "]\n",
    "exclude_for_tables_filt = dol.filt_iter(\n",
    "    filt=lambda x: not any(re.match(p, x) for p in exclude_for_tables)\n",
    ")\n",
    "\n",
    "\n",
    "list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis and WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking through the code_folders_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're going to look through the `code_folders_analysis` to see if everything is fine.\n",
    "\n",
    "(When it's not, we address it and include the logic above so we get it all at once good.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_entries_with_empty_url_or_urls(code_folders_analysis):\n",
    "    return [i for i, d in enumerate(code_folders_analysis) if not d['url'] and not d['urls']]\n",
    "\n",
    "t = get_entries_with_empty_url_or_urls(code_folders_analysis)\n",
    "print(t)\n",
    "[code_folders_analysis[i]['code'] for i in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imbed_data_prep.eurovis\n",
    "\n",
    "known_urls = {\n",
    "    'hcp.py': {\n",
    "        'url': 'HCP_PUBS_SRC_KEY',\n",
    "        'url_type': 'env_var',\n",
    "    },\n",
    "    'eurovis.py': {\n",
    "        'url': 'https://81593031860c2ee4ad53a08892f7e95d.r2.cloudflarestorage.com/cosmograph/projects/eurovis/raw_data.csv',\n",
    "        'url_type': 'http',\n",
    "        'filepath': '/Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/eurovis/raw_data.csv'\n",
    "    },\n",
    "    # 'embeddings_of_aggregations.py': {\"NOT A DATA PREP MODULE\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- i=3 -----\n",
      "{'code': 'hcp.py',\n",
      " 'data_keys': {'aggregate_titles_embeddings_key': 'Filepath for aggregated '\n",
      "                                                  'titles embeddings.',\n",
      "               'citations_src_key': 'Filepath for citations data.',\n",
      "               'embeddings_src_key': 'Filepath for embedding data.',\n",
      "               'info_src_key': 'Filepath for additional publication info.',\n",
      "               'src_key': 'Path to source key based on local config or '\n",
      "                          'environment variable.'},\n",
      " 'description': 'NOT DATA PREP CODE: This code primarily defines a class to '\n",
      "                'handle various operations on HCP (Human Connectome Project) '\n",
      "                'publications data and their citations. It retrieves and '\n",
      "                'processes embeddings and citation data, but it does not '\n",
      "                'include explicit downloading or acquiring of raw data from an '\n",
      "                'external source. The only potential environment variable '\n",
      "                'referenced is for a local configuration, suggesting that raw '\n",
      "                'data may need to be present locally.',\n",
      " 'functions': {'aggregate_values': 'Aggregates values in a dictionary based on '\n",
      "                                   'specified aggregation functions.',\n",
      "               'citation_graph': 'Constructs a citation graph filtered by '\n",
      "                                 'minimum citation requirements.',\n",
      "               'embeddings_for_ids': 'Retrieves embeddings for specified '\n",
      "                                     'article IDs from a DataFrame.',\n",
      "               'get_src_key_from_local_configs': 'Fetches the source key from '\n",
      "                                                 'environment variables.',\n",
      "               'titles_aggregate': 'Aggregates titles of cited papers for each '\n",
      "                                   'published article.',\n",
      "               'titles_aggregate_sr': 'Streamlined version of titles_aggregate '\n",
      "                                      'yielding results as a pandas Series.'},\n",
      " 'name': 'HCP Publications Analysis',\n",
      " 'parameters': {'main_title_sep': 'Separator used for the main title in '\n",
      "                                  'aggregated titles.',\n",
      "                'min_citations': 'Minimum number of citations required for an '\n",
      "                                 'article to be included in the citation '\n",
      "                                 'graph.',\n",
      "                'min_proportion': 'Minimum proportion of citations that must '\n",
      "                                  'have embeddings for a node to be included '\n",
      "                                  'in the graph.',\n",
      "                'print_progress_every': 'Frequency to print progress during '\n",
      "                                        'the aggregation of titles.',\n",
      "                'titles_sep': 'Separator used for titles in aggregated '\n",
      "                              'titles.'},\n",
      " 'url': '',\n",
      " 'url_type': '',\n",
      " 'urls': {}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "print(f\"--- {i=} -----\")\n",
    "t = code_folders_analysis[i]\n",
    "if isinstance(t, str):\n",
    "    import json \n",
    "    json = json.loads(t)\n",
    "pprint(t)\n",
    "i = (i + 1) % len(code_folders_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
