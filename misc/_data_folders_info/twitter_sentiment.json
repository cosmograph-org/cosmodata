{
  "project_group": "imbed_saves",
  "data_filenames": [
    "twitter_sentiment.zip"
  ],
  "data_filepaths": {
    "/Users/thorwhalen/Dropbox/_odata/app_data/imbed/saves/twitter_sentiment/twitter_sentiment.zip": 84855679
  },
  "code_file": "twitter_sentiment.py",
  "code_contents": "\"\"\"Data prep for Eurovis data.\"\"\"\n\nfrom functools import partial\nimport os\nfrom dataclasses import dataclass, KW_ONLY\nfrom typing import Optional\n\nfrom dol import cache_this as _cache_this, add_extension\nfrom imbed.util import (\n    saves_join,\n    planar_embeddings,\n    planar_embeddings_dict_to_df,\n)\n\n\nimport pandas as pd\n\n\ndata_name = 'twitter_sentiment'\n\n# raw_data_name = 'github_repos.parquet'\n\n# TODO: Use config2py tools to use and save default values\n# TODO: Use config2py tools to include a message containing the default values\n_DFLT_CACHE_DIR = saves_join(data_name)\n\nDFLT_CACHE_DIR = os.environ.get('TWITTER_SENTIMENT_DATA_DIR', default=_DFLT_CACHE_DIR)\n\nDFLT_N_CLUSTERS = (5, 8, 13, 21, 34)\n\n\n# TODO: _cache_this doesn't work well with partial\n# cache_this = partial(_cache_this, cache='cache')\ncache_this = partial(_cache_this, cache='saves', key=add_extension('parquet'))\n\nimport oa\nfrom dataclasses import dataclass, KW_ONLY\nfrom imbed.base import (\n    LocalSavesMixin,\n    DFLT_SAVES_DIR,\n    DFLT_EMBEDDING_MODEL,\n)\nfrom imbed.data_prep import ImbedArtifactsMixin\n\n\n# TODO: Move embed_segments_one_by_one to reusables (e.g. imbed)\nfrom typing import Mapping, Generator\n\nKeyVectorPairs = Generator[tuple[str, list[float]], None, None]\n\n\ndef embed_segments_one_by_one(segments: Mapping[str, str]) -> KeyVectorPairs:\n    print(f\"This could take a LONG TIME!!!\")\n    assert isinstance(segments, dict)\n    for i, (key, segment) in enumerate(segments.items()):\n        try:\n            vector = oa.embeddings(segment)\n            yield key, vector\n        except Exception as e:\n            print(f\"Error processing ({i=}) {key}: {e}\")\n            # continue\n\n\nimport pandas as pd\nfrom haggle import KaggleDatasets\nfrom imbed import extension_based_wrap\n\n\n@dataclass\nclass Dacc(LocalSavesMixin, ImbedArtifactsMixin):\n    name: Optional[str] = data_name\n    _: KW_ONLY\n    saves_dir: str = os.path.join(DFLT_SAVES_DIR, data_name)\n    verbose: int = 1\n    model: str = DFLT_EMBEDDING_MODEL\n\n    column_info = {\n        \"target\": \"the polarity of the tweet (e.g. 0 = negative, 2 = neutral, 4 = positive)\",\n        \"id_\": \"The id of the tweet (e.g. 2087)\",\n        \"date\": \"the date of the tweet (e.g. Sat May 16 23:58:44 UTC 2009)\",\n        \"flag\": \"The query (lyx). If there is no query, then this value is NO_QUERY.\",\n        \"user\": \"the user that tweeted (e.g. robotickilldozr)\",\n        \"text\": \"the text of the tweet (e.g. Lyx is cool)\",\n    }\n\n    @cache_this(cache=None)\n    def raw_data(self):\n        \"\"\"The purpose of this function is to return the data that the user would\n        have manually place in the saved_dir, so the method should never actually\n        be called.\"\"\"\n\n        kaggle = KaggleDatasets()\n\n        raw_data_store = extension_based_wrap(kaggle['kazanova/sentiment140'])\n        list(raw_data_store)\n        df = raw_data_store['training.1600000.processed.noemoticon.csv']\n\n        # there was no header in the csv file, so we need to specify the column names\n        columns = list(\n            self.column_info.keys()\n        )  # (was extracted from https://www.kaggle.com/datasets/kazanova/sentiment140)\n        # we need to move the current columns of df to be the first row,\n        #  and then set the columns to be the columns we want\n        df = pd.DataFrame([df.columns] + df.values.tolist(), columns=columns)\n\n        return df\n\n    # @cache_this(pre_cache=True)\n    # def embeddable(self):\n    #     # remove rows with missing title or abstract\n    #     df = self.raw_data.dropna(subset=['title', 'abstract'])\n    #     ## Add some util columns\n    #     # contatenate title and abstract to make segment\n    #     df['segment'] = '##' + df['title'] + '\\n\\n' + df.get('abstract')\n\n    #     assert not any(df.segment.isna()), \"some segments are NaN\"\n\n    #     # apply oa.num_tokens to segment to get n_tokens\n    #     df['n_tokens'] = df['segment'].apply(oa.num_tokens)\n    #     max_tokens = oa.util.embeddings_models[oa.base.DFLT_EMBEDDINGS_MODEL][\n    #         'max_input'\n    #     ]\n    #     assert df['n_tokens'].max() <= max_tokens, \"some segments exceed max tokens\"\n\n    #     df.set_index('doi', drop=False, inplace=True)\n    #     df.index.name = 'id_'\n    #     return df\n\n    # # Replace by more scalable default\n    # @cache_this\n    # def embeddings_df(self):\n    #     \"\"\"WARNING: This could take A LONG TIME\"\"\"\n    #     segments = self.segments()  # TODO: Should detect if property or method!\n    #     vectors = dict(embed_segments_one_by_one(segments))\n    #     df = pd.DataFrame(vectors).T\n    #     df.index.name = 'id_'\n    #     return df\n\n    # @cache_this\n    # def planar_embeddings(self):\n    #     # TODO: Test this\n    #     d = planar_embeddings(self.embeddings_df)\n    #     # make it into a dataframe with the same index as the embeddings_df\n    #     return planar_embeddings_dict_to_df(d)\n\n    # @property\n    # def segments(self):\n    #     df = self.embeddable\n    #     segments = dict(zip(df.index.values, df.segment))\n    #     assert len(segments) == len(df), \"oops, duplicate DOIs\"\n    #     assert all(\n    #         map(oa.text_is_valid, df.segment.values)\n    #     ), \"some segments are invalid\"\n\n    #     return segments\n\n    # @cache_this\n    # def clusters_df(self):\n    #     return super().clusters_df()\n\n    # @cache_this(cache='saves', key=add_extension('parquet'))\n    # def merged_artifacts(self):\n    #     t = self.embeddable\n    #     t = t.merge(self.planar_embeddings, left_index=True, right_index=True)\n    #     t = t.merge(self.clusters_df, left_index=True, right_index=True)\n    #     return t\n\n    # @cache_this\n    # def testing123(self):\n    #     return pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
  "num_of_data_files": 1,
  "tables_info": {
    "twitter_sentiment.zip": null
  }
}